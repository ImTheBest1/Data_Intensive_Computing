cur_tweetsDF <-cbind(cur_tweetsDF,geo1)
}
cur_tweetsDF <- subset(cur_tweetsDF, cur_tweetsDF$country == "United States of America")
#-----------------------step5-1: add cur_tweetsDF to tweetsDF.csv files-------
temp1 <- cur_tweetsDF[, c("id", "long", "lat", "text")]
# combine two csv files
final1DF <- rbind(temp1, pre_tweetsDF)
# remove the data if they are all the same
dropX <-"X"
final1DF <-final1DF[ , !(names(final1DF) %in% dropX)]
write.csv(final1DF, file = "tweetsDF.csv")
#-----------------------step5-2: add cur_tweetsDF to updatedTweets.csv files-------
temp2 <- cur_tweetsDF[,c("id", "long", "lat", "states")]
pre_tweetsDF <- read.csv("updatedTweets.csv", header=T, sep=",")
dropX <-"X"
pre_tweetsDF <-pre_tweetsDF[ , !(names(pre_tweetsDF) %in% dropX)]
final2DF <- rbind(temp2, pre_tweetsDF)
write.csv(final2DF, file = "updatedTweets.csv")
cur_tweets <- searchTwitter("cold flu", n=5000)
# convert twitteR lists to data frame
cur_tweetsDF <- twListToDF(cur_tweets)
# store no location
noLocation <- cur_tweetsDF[(is.na(cur_tweetsDF$longitude) | is.na(cur_tweetsDF$latitude)),]
# store the tweets Data frame has location
cur_tweetsDF <- cur_tweetsDF[!(is.na(cur_tweetsDF$longitude) & is.na(cur_tweetsDF$latitude)),]
for(row in 1:nrow(noLocation)){
usr_id <- noLocation[row, "id"]
usr_screenName <- noLocation[row, "screenName"]
usrs <- lookupUsers(c(usr_id, usr_screenName))
temp <- twListToDF(usrs)
loc_str <- temp$location
geo <- geocode(location = loc_str)
long <- geo$lon
lat <- geo$lat
noLocation[row, "longitude"] <- long
noLocation[row, "latitude"] <- lat
}
cur_tweetsDF <- rbind(cur_tweetsDF, noLocation)
cur_tweetsDF <- cur_tweetsDF[!(is.na(cur_tweetsDF$longitude) & is.na(cur_tweetsDF$latitude)),]
if(nrow(cur_tweetsDF) > 0){
# store the tweets has text, location, and id
cur_tweetsDF <- cur_tweetsDF[c("id","longitude", "latitude", "text")]
colnames(cur_tweetsDF)[2] <- "long"
colnames(cur_tweetsDF)[3] <- "lat"
cur_tweetsDF$coder <-"B"
cur_tweetsDF <-unique(cur_tweetsDF)
}
pre_tweetsDF <- read.csv("tweetsDF.csv", header=T, sep=",")
pre_tweetsDF <- pre_tweetsDF[c("id","long", "lat", "text")]
pre_tweetsDF$coder <- "A"
# create a tempDF to store row combine two data frame
tempDF <-  rbind(pre_tweetsDF, cur_tweetsDF)
tempDF <- tempDF[, c("coder", "id", "long", "lat", "text")]
# Find the rows which have duplicates in a different group.
dupRows <- dupsBetweenGroups(tempDF, "coder")
# Print it alongside the data frame
tempDF <- cbind(tempDF, dup=dupRows)
# split two data frame, in ordering to find if there is duplicated in cur_tweetsDF
pre_tweetsDF <- subset(tempDF, coder=="A", select=-c(coder, dup))
cur_tweetsDF <- subset(tempDF, coder=="B", select = -coder)
# remove the row when dup== true, the row exists in pre_tweetsDF as well
cur_tweetsDF <- subset(cur_tweetsDF, dup=="FALSE")
cur_tweetsDF <- cur_tweetsDF[, c("id", "long", "lat", "text")]
cur_tweetsDF$long <- as.numeric(as.character(cur_tweetsDF$long))
cur_tweetsDF$lat <- as.numeric(as.character(cur_tweetsDF$lat))
cur_tweetsDF <- subset(cur_tweetsDF,cur_tweetsDF$long > (-180) & cur_tweetsDF$long < 0 & cur_tweetsDF$lat > 0 & cur_tweetsDF$lat < 180)
debug1 <- cur_tweetsDF
# cur_tweetsDF <- debug1
if(nrow(cur_tweetsDF) != 0){
# cur_tweetsDF$long <- as.character(cur_tweetsDF$long)
# cur_tweetsDF$lat <- as.character(cur_tweetsDF$lat)
geo1 <- getGeoLocation(cur_tweetsDF[1, "long"], cur_tweetsDF[1,"lat"])
if(nrow(cur_tweetsDF) > 1){
for (row in 2:nrow(cur_tweetsDF)) {
geon <- getGeoLocation(cur_tweetsDF[row, "long"], cur_tweetsDF[row, "lat"])
geo1 <- rbind(geo1, geon)
}
}
geo1 <- geo1[, c("state", "country")]
colnames(geo1)[1] <- "states"
cur_tweetsDF <- cur_tweetsDF[c("id","long", "lat", "text")]
cur_tweetsDF <-cbind(cur_tweetsDF,geo1)
}
cur_tweetsDF <- subset(cur_tweetsDF, cur_tweetsDF$country == "United States of America")
#-----------------------step5-1: add cur_tweetsDF to tweetsDF.csv files-------
temp1 <- cur_tweetsDF[, c("id", "long", "lat", "text")]
# combine two csv files
final1DF <- rbind(temp1, pre_tweetsDF)
# remove the data if they are all the same
dropX <-"X"
final1DF <-final1DF[ , !(names(final1DF) %in% dropX)]
write.csv(final1DF, file = "tweetsDF.csv")
#-----------------------step5-2: add cur_tweetsDF to updatedTweets.csv files-------
temp2 <- cur_tweetsDF[,c("id", "long", "lat", "states")]
pre_tweetsDF <- read.csv("updatedTweets.csv", header=T, sep=",")
dropX <-"X"
pre_tweetsDF <-pre_tweetsDF[ , !(names(pre_tweetsDF) %in% dropX)]
final2DF <- rbind(temp2, pre_tweetsDF)
write.csv(final2DF, file = "updatedTweets.csv")
cur_tweets <- searchTwitter("cold", n=5000)
# convert twitteR lists to data frame
cur_tweetsDF <- twListToDF(cur_tweets)
# store no location
noLocation <- cur_tweetsDF[(is.na(cur_tweetsDF$longitude) | is.na(cur_tweetsDF$latitude)),]
# store the tweets Data frame has location
cur_tweetsDF <- cur_tweetsDF[!(is.na(cur_tweetsDF$longitude) & is.na(cur_tweetsDF$latitude)),]
for(row in 1:nrow(noLocation)){
usr_id <- noLocation[row, "id"]
usr_screenName <- noLocation[row, "screenName"]
usrs <- lookupUsers(c(usr_id, usr_screenName))
temp <- twListToDF(usrs)
loc_str <- temp$location
geo <- geocode(location = loc_str)
long <- geo$lon
lat <- geo$lat
noLocation[row, "longitude"] <- long
noLocation[row, "latitude"] <- lat
}
cur_tweetsDF <- rbind(cur_tweetsDF, noLocation)
cur_tweetsDF <- cur_tweetsDF[!(is.na(cur_tweetsDF$longitude) & is.na(cur_tweetsDF$latitude)),]
if(nrow(cur_tweetsDF) > 0){
# store the tweets has text, location, and id
cur_tweetsDF <- cur_tweetsDF[c("id","longitude", "latitude", "text")]
colnames(cur_tweetsDF)[2] <- "long"
colnames(cur_tweetsDF)[3] <- "lat"
cur_tweetsDF$coder <-"B"
cur_tweetsDF <-unique(cur_tweetsDF)
}
# read previous tweets data frame csv file
pre_tweetsDF <- read.csv("tweetsDF.csv", header=T, sep=",")
pre_tweetsDF <- pre_tweetsDF[c("id","long", "lat", "text")]
pre_tweetsDF$coder <- "A"
# create a tempDF to store row combine two data frame
tempDF <-  rbind(pre_tweetsDF, cur_tweetsDF)
tempDF <- tempDF[, c("coder", "id", "long", "lat", "text")]
# Find the rows which have duplicates in a different group.
dupRows <- dupsBetweenGroups(tempDF, "coder")
# Print it alongside the data frame
tempDF <- cbind(tempDF, dup=dupRows)
# split two data frame, in ordering to find if there is duplicated in cur_tweetsDF
pre_tweetsDF <- subset(tempDF, coder=="A", select=-c(coder, dup))
cur_tweetsDF <- subset(tempDF, coder=="B", select = -coder)
# remove the row when dup== true, the row exists in pre_tweetsDF as well
cur_tweetsDF <- subset(cur_tweetsDF, dup=="FALSE")
cur_tweetsDF <- cur_tweetsDF[, c("id", "long", "lat", "text")]
#-----------step4: search the cur_tweetsDF geolocation inside the USA-----
cur_tweetsDF$long <- as.numeric(as.character(cur_tweetsDF$long))
cur_tweetsDF$lat <- as.numeric(as.character(cur_tweetsDF$lat))
cur_tweetsDF <- subset(cur_tweetsDF,cur_tweetsDF$long > (-180) & cur_tweetsDF$long < 0 & cur_tweetsDF$lat > 0 & cur_tweetsDF$lat < 180)
debug1 <- cur_tweetsDF
# cur_tweetsDF <- debug1
if(nrow(cur_tweetsDF) != 0){
# cur_tweetsDF$long <- as.character(cur_tweetsDF$long)
# cur_tweetsDF$lat <- as.character(cur_tweetsDF$lat)
geo1 <- getGeoLocation(cur_tweetsDF[1, "long"], cur_tweetsDF[1,"lat"])
if(nrow(cur_tweetsDF) > 1){
for (row in 2:nrow(cur_tweetsDF)) {
geon <- getGeoLocation(cur_tweetsDF[row, "long"], cur_tweetsDF[row, "lat"])
geo1 <- rbind(geo1, geon)
}
}
geo1 <- geo1[, c("state", "country")]
colnames(geo1)[1] <- "states"
cur_tweetsDF <- cur_tweetsDF[c("id","long", "lat", "text")]
cur_tweetsDF <-cbind(cur_tweetsDF,geo1)
}
cur_tweetsDF <- subset(cur_tweetsDF, cur_tweetsDF$country == "United States of America")
#-----------------------step5-1: add cur_tweetsDF to tweetsDF.csv files-------
temp1 <- cur_tweetsDF[, c("id", "long", "lat", "text")]
# combine two csv files
final1DF <- rbind(temp1, pre_tweetsDF)
# remove the data if they are all the same
dropX <-"X"
final1DF <-final1DF[ , !(names(final1DF) %in% dropX)]
write.csv(final1DF, file = "tweetsDF.csv")
#-----------------------step5-2: add cur_tweetsDF to updatedTweets.csv files-------
temp2 <- cur_tweetsDF[,c("id", "long", "lat", "states")]
pre_tweetsDF <- read.csv("updatedTweets.csv", header=T, sep=",")
dropX <-"X"
pre_tweetsDF <-pre_tweetsDF[ , !(names(pre_tweetsDF) %in% dropX)]
final2DF <- rbind(temp2, pre_tweetsDF)
write.csv(final2DF, file = "updatedTweets.csv")
cur_tweets <- searchTwitter("sick", n=5000)
# convert twitteR lists to data frame
cur_tweetsDF <- twListToDF(cur_tweets)
# store no location
noLocation <- cur_tweetsDF[(is.na(cur_tweetsDF$longitude) | is.na(cur_tweetsDF$latitude)),]
# store the tweets Data frame has location
cur_tweetsDF <- cur_tweetsDF[!(is.na(cur_tweetsDF$longitude) & is.na(cur_tweetsDF$latitude)),]
for(row in 1:nrow(noLocation)){
usr_id <- noLocation[row, "id"]
usr_screenName <- noLocation[row, "screenName"]
usrs <- lookupUsers(c(usr_id, usr_screenName))
temp <- twListToDF(usrs)
loc_str <- temp$location
geo <- geocode(location = loc_str)
long <- geo$lon
lat <- geo$lat
noLocation[row, "longitude"] <- long
noLocation[row, "latitude"] <- lat
}
cur_tweetsDF <- rbind(cur_tweetsDF, noLocation)
cur_tweetsDF <- cur_tweetsDF[!(is.na(cur_tweetsDF$longitude) & is.na(cur_tweetsDF$latitude)),]
if(nrow(cur_tweetsDF) > 0){
# store the tweets has text, location, and id
cur_tweetsDF <- cur_tweetsDF[c("id","longitude", "latitude", "text")]
colnames(cur_tweetsDF)[2] <- "long"
colnames(cur_tweetsDF)[3] <- "lat"
cur_tweetsDF$coder <-"B"
cur_tweetsDF <-unique(cur_tweetsDF)
}
#------------step3: find if there is duplicates value with previous data we searched----------------------
# read previous tweets data frame csv file
pre_tweetsDF <- read.csv("tweetsDF.csv", header=T, sep=",")
pre_tweetsDF <- pre_tweetsDF[c("id","long", "lat", "text")]
pre_tweetsDF$coder <- "A"
# create a tempDF to store row combine two data frame
tempDF <-  rbind(pre_tweetsDF, cur_tweetsDF)
tempDF <- tempDF[, c("coder", "id", "long", "lat", "text")]
# Find the rows which have duplicates in a different group.
dupRows <- dupsBetweenGroups(tempDF, "coder")
# Print it alongside the data frame
tempDF <- cbind(tempDF, dup=dupRows)
# split two data frame, in ordering to find if there is duplicated in cur_tweetsDF
pre_tweetsDF <- subset(tempDF, coder=="A", select=-c(coder, dup))
cur_tweetsDF <- subset(tempDF, coder=="B", select = -coder)
# remove the row when dup== true, the row exists in pre_tweetsDF as well
cur_tweetsDF <- subset(cur_tweetsDF, dup=="FALSE")
cur_tweetsDF <- cur_tweetsDF[, c("id", "long", "lat", "text")]
#-----------step4: search the cur_tweetsDF geolocation inside the USA-----
cur_tweetsDF$long <- as.numeric(as.character(cur_tweetsDF$long))
cur_tweetsDF$lat <- as.numeric(as.character(cur_tweetsDF$lat))
cur_tweetsDF <- subset(cur_tweetsDF,cur_tweetsDF$long > (-180) & cur_tweetsDF$long < 0 & cur_tweetsDF$lat > 0 & cur_tweetsDF$lat < 180)
debug1 <- cur_tweetsDF
# cur_tweetsDF <- debug1
if(nrow(cur_tweetsDF) != 0){
# cur_tweetsDF$long <- as.character(cur_tweetsDF$long)
# cur_tweetsDF$lat <- as.character(cur_tweetsDF$lat)
geo1 <- getGeoLocation(cur_tweetsDF[1, "long"], cur_tweetsDF[1,"lat"])
if(nrow(cur_tweetsDF) > 1){
for (row in 2:nrow(cur_tweetsDF)) {
geon <- getGeoLocation(cur_tweetsDF[row, "long"], cur_tweetsDF[row, "lat"])
geo1 <- rbind(geo1, geon)
}
}
geo1 <- geo1[, c("state", "country")]
colnames(geo1)[1] <- "states"
cur_tweetsDF <- cur_tweetsDF[c("id","long", "lat", "text")]
cur_tweetsDF <-cbind(cur_tweetsDF,geo1)
}
cur_tweetsDF <- subset(cur_tweetsDF, cur_tweetsDF$country == "United States of America")
#-----------------------step5-1: add cur_tweetsDF to tweetsDF.csv files-------
temp1 <- cur_tweetsDF[, c("id", "long", "lat", "text")]
# combine two csv files
final1DF <- rbind(temp1, pre_tweetsDF)
# remove the data if they are all the same
dropX <-"X"
final1DF <-final1DF[ , !(names(final1DF) %in% dropX)]
write.csv(final1DF, file = "tweetsDF.csv")
#-----------------------step5-2: add cur_tweetsDF to updatedTweets.csv files-------
temp2 <- cur_tweetsDF[,c("id", "long", "lat", "states")]
pre_tweetsDF <- read.csv("updatedTweets.csv", header=T, sep=",")
dropX <-"X"
pre_tweetsDF <-pre_tweetsDF[ , !(names(pre_tweetsDF) %in% dropX)]
final2DF <- rbind(temp2, pre_tweetsDF)
write.csv(final2DF, file = "updatedTweets.csv")
cur_tweets <- searchTwitter("influenza", n=5000)
# convert twitteR lists to data frame
cur_tweetsDF <- twListToDF(cur_tweets)
# store no location
noLocation <- cur_tweetsDF[(is.na(cur_tweetsDF$longitude) | is.na(cur_tweetsDF$latitude)),]
# store the tweets Data frame has location
cur_tweetsDF <- cur_tweetsDF[!(is.na(cur_tweetsDF$longitude) & is.na(cur_tweetsDF$latitude)),]
for(row in 1:nrow(noLocation)){
usr_id <- noLocation[row, "id"]
usr_screenName <- noLocation[row, "screenName"]
usrs <- lookupUsers(c(usr_id, usr_screenName))
temp <- twListToDF(usrs)
loc_str <- temp$location
geo <- geocode(location = loc_str)
long <- geo$lon
lat <- geo$lat
noLocation[row, "longitude"] <- long
noLocation[row, "latitude"] <- lat
}
cur_tweetsDF <- rbind(cur_tweetsDF, noLocation)
cur_tweetsDF <- cur_tweetsDF[!(is.na(cur_tweetsDF$longitude) & is.na(cur_tweetsDF$latitude)),]
if(nrow(cur_tweetsDF) > 0){
# store the tweets has text, location, and id
cur_tweetsDF <- cur_tweetsDF[c("id","longitude", "latitude", "text")]
colnames(cur_tweetsDF)[2] <- "long"
colnames(cur_tweetsDF)[3] <- "lat"
cur_tweetsDF$coder <-"B"
cur_tweetsDF <-unique(cur_tweetsDF)
}
pre_tweetsDF <- read.csv("tweetsDF.csv", header=T, sep=",")
pre_tweetsDF <- pre_tweetsDF[c("id","long", "lat", "text")]
pre_tweetsDF$coder <- "A"
# create a tempDF to store row combine two data frame
tempDF <-  rbind(pre_tweetsDF, cur_tweetsDF)
tempDF <- tempDF[, c("coder", "id", "long", "lat", "text")]
# Find the rows which have duplicates in a different group.
dupRows <- dupsBetweenGroups(tempDF, "coder")
# Print it alongside the data frame
tempDF <- cbind(tempDF, dup=dupRows)
# split two data frame, in ordering to find if there is duplicated in cur_tweetsDF
pre_tweetsDF <- subset(tempDF, coder=="A", select=-c(coder, dup))
cur_tweetsDF <- subset(tempDF, coder=="B", select = -coder)
# remove the row when dup== true, the row exists in pre_tweetsDF as well
cur_tweetsDF <- subset(cur_tweetsDF, dup=="FALSE")
cur_tweetsDF <- cur_tweetsDF[, c("id", "long", "lat", "text")]
#-----------step4: search the cur_tweetsDF geolocation inside the USA-----
cur_tweetsDF$long <- as.numeric(as.character(cur_tweetsDF$long))
cur_tweetsDF$lat <- as.numeric(as.character(cur_tweetsDF$lat))
cur_tweetsDF <- subset(cur_tweetsDF,cur_tweetsDF$long > (-180) & cur_tweetsDF$long < 0 & cur_tweetsDF$lat > 0 & cur_tweetsDF$lat < 180)
debug1 <- cur_tweetsDF
# cur_tweetsDF <- debug1
if(nrow(cur_tweetsDF) != 0){
# cur_tweetsDF$long <- as.character(cur_tweetsDF$long)
# cur_tweetsDF$lat <- as.character(cur_tweetsDF$lat)
geo1 <- getGeoLocation(cur_tweetsDF[1, "long"], cur_tweetsDF[1,"lat"])
if(nrow(cur_tweetsDF) > 1){
for (row in 2:nrow(cur_tweetsDF)) {
geon <- getGeoLocation(cur_tweetsDF[row, "long"], cur_tweetsDF[row, "lat"])
geo1 <- rbind(geo1, geon)
}
}
geo1 <- geo1[, c("state", "country")]
colnames(geo1)[1] <- "states"
cur_tweetsDF <- cur_tweetsDF[c("id","long", "lat", "text")]
cur_tweetsDF <-cbind(cur_tweetsDF,geo1)
}
cur_tweetsDF <- subset(cur_tweetsDF, cur_tweetsDF$country == "United States of America")
#-----------------------step5-1: add cur_tweetsDF to tweetsDF.csv files-------
temp1 <- cur_tweetsDF[, c("id", "long", "lat", "text")]
# combine two csv files
final1DF <- rbind(temp1, pre_tweetsDF)
# remove the data if they are all the same
dropX <-"X"
final1DF <-final1DF[ , !(names(final1DF) %in% dropX)]
write.csv(final1DF, file = "tweetsDF.csv")
#-----------------------step5-2: add cur_tweetsDF to updatedTweets.csv files-------
temp2 <- cur_tweetsDF[,c("id", "long", "lat", "states")]
pre_tweetsDF <- read.csv("updatedTweets.csv", header=T, sep=",")
dropX <-"X"
pre_tweetsDF <-pre_tweetsDF[ , !(names(pre_tweetsDF) %in% dropX)]
final2DF <- rbind(temp2, pre_tweetsDF)
write.csv(final2DF, file = "updatedTweets.csv")
cur_tweets <- searchTwitter("sick", n=5000)
# convert twitteR lists to data frame
cur_tweetsDF <- twListToDF(cur_tweets)
# store no location
noLocation <- cur_tweetsDF[(is.na(cur_tweetsDF$longitude) | is.na(cur_tweetsDF$latitude)),]
# store the tweets Data frame has location
cur_tweetsDF <- cur_tweetsDF[!(is.na(cur_tweetsDF$longitude) & is.na(cur_tweetsDF$latitude)),]
for(row in 1:nrow(noLocation)){
usr_id <- noLocation[row, "id"]
usr_screenName <- noLocation[row, "screenName"]
usrs <- lookupUsers(c(usr_id, usr_screenName))
temp <- twListToDF(usrs)
loc_str <- temp$location
geo <- geocode(location = loc_str)
long <- geo$lon
lat <- geo$lat
noLocation[row, "longitude"] <- long
noLocation[row, "latitude"] <- lat
}
cur_tweetsDF <- rbind(cur_tweetsDF, noLocation)
cur_tweetsDF <- cur_tweetsDF[!(is.na(cur_tweetsDF$longitude) & is.na(cur_tweetsDF$latitude)),]
if(nrow(cur_tweetsDF) > 0){
# store the tweets has text, location, and id
cur_tweetsDF <- cur_tweetsDF[c("id","longitude", "latitude", "text")]
colnames(cur_tweetsDF)[2] <- "long"
colnames(cur_tweetsDF)[3] <- "lat"
cur_tweetsDF$coder <-"B"
cur_tweetsDF <-unique(cur_tweetsDF)
}
pre_tweetsDF <- read.csv("tweetsDF.csv", header=T, sep=",")
pre_tweetsDF <- pre_tweetsDF[c("id","long", "lat", "text")]
pre_tweetsDF$coder <- "A"
# create a tempDF to store row combine two data frame
tempDF <-  rbind(pre_tweetsDF, cur_tweetsDF)
tempDF <- tempDF[, c("coder", "id", "long", "lat", "text")]
# Find the rows which have duplicates in a different group.
dupRows <- dupsBetweenGroups(tempDF, "coder")
# Print it alongside the data frame
tempDF <- cbind(tempDF, dup=dupRows)
# split two data frame, in ordering to find if there is duplicated in cur_tweetsDF
pre_tweetsDF <- subset(tempDF, coder=="A", select=-c(coder, dup))
cur_tweetsDF <- subset(tempDF, coder=="B", select = -coder)
# remove the row when dup== true, the row exists in pre_tweetsDF as well
cur_tweetsDF <- subset(cur_tweetsDF, dup=="FALSE")
cur_tweetsDF <- cur_tweetsDF[, c("id", "long", "lat", "text")]
#-----------step4: search the cur_tweetsDF geolocation inside the USA-----
cur_tweetsDF$long <- as.numeric(as.character(cur_tweetsDF$long))
cur_tweetsDF$lat <- as.numeric(as.character(cur_tweetsDF$lat))
cur_tweetsDF <- subset(cur_tweetsDF,cur_tweetsDF$long > (-180) & cur_tweetsDF$long < 0 & cur_tweetsDF$lat > 0 & cur_tweetsDF$lat < 180)
debug1 <- cur_tweetsDF
# cur_tweetsDF <- debug1
if(nrow(cur_tweetsDF) != 0){
# cur_tweetsDF$long <- as.character(cur_tweetsDF$long)
# cur_tweetsDF$lat <- as.character(cur_tweetsDF$lat)
geo1 <- getGeoLocation(cur_tweetsDF[1, "long"], cur_tweetsDF[1,"lat"])
if(nrow(cur_tweetsDF) > 1){
for (row in 2:nrow(cur_tweetsDF)) {
geon <- getGeoLocation(cur_tweetsDF[row, "long"], cur_tweetsDF[row, "lat"])
geo1 <- rbind(geo1, geon)
}
}
geo1 <- geo1[, c("state", "country")]
colnames(geo1)[1] <- "states"
cur_tweetsDF <- cur_tweetsDF[c("id","long", "lat", "text")]
cur_tweetsDF <-cbind(cur_tweetsDF,geo1)
}
cur_tweetsDF <- subset(cur_tweetsDF, cur_tweetsDF$country == "United States of America")
#-----------------------step5-1: add cur_tweetsDF to tweetsDF.csv files-------
temp1 <- cur_tweetsDF[, c("id", "long", "lat", "text")]
# combine two csv files
final1DF <- rbind(temp1, pre_tweetsDF)
# remove the data if they are all the same
dropX <-"X"
final1DF <-final1DF[ , !(names(final1DF) %in% dropX)]
write.csv(final1DF, file = "tweetsDF.csv")
#-----------------------step5-2: add cur_tweetsDF to updatedTweets.csv files-------
temp2 <- cur_tweetsDF[,c("id", "long", "lat", "states")]
pre_tweetsDF <- read.csv("updatedTweets.csv", header=T, sep=",")
dropX <-"X"
pre_tweetsDF <-pre_tweetsDF[ , !(names(pre_tweetsDF) %in% dropX)]
final2DF <- rbind(temp2, pre_tweetsDF)
write.csv(final2DF, file = "updatedTweets.csv")
cur_tweets <- searchTwitter("sick", n=5000)
# convert twitteR lists to data frame
cur_tweetsDF <- twListToDF(cur_tweets)
# store no location
noLocation <- cur_tweetsDF[(is.na(cur_tweetsDF$longitude) | is.na(cur_tweetsDF$latitude)),]
# store the tweets Data frame has location
cur_tweetsDF <- cur_tweetsDF[!(is.na(cur_tweetsDF$longitude) & is.na(cur_tweetsDF$latitude)),]
for(row in 1:nrow(noLocation)){
usr_id <- noLocation[row, "id"]
usr_screenName <- noLocation[row, "screenName"]
usrs <- lookupUsers(c(usr_id, usr_screenName))
temp <- twListToDF(usrs)
loc_str <- temp$location
geo <- geocode(location = loc_str)
long <- geo$lon
lat <- geo$lat
noLocation[row, "longitude"] <- long
noLocation[row, "latitude"] <- lat
}
cur_tweetsDF <- rbind(cur_tweetsDF, noLocation)
cur_tweetsDF <- cur_tweetsDF[!(is.na(cur_tweetsDF$longitude) & is.na(cur_tweetsDF$latitude)),]
if(nrow(cur_tweetsDF) > 0){
# store the tweets has text, location, and id
cur_tweetsDF <- cur_tweetsDF[c("id","longitude", "latitude", "text")]
colnames(cur_tweetsDF)[2] <- "long"
colnames(cur_tweetsDF)[3] <- "lat"
cur_tweetsDF$coder <-"B"
cur_tweetsDF <-unique(cur_tweetsDF)
}
pre_tweetsDF <- read.csv("tweetsDF.csv", header=T, sep=",")
pre_tweetsDF <- pre_tweetsDF[c("id","long", "lat", "text")]
pre_tweetsDF$coder <- "A"
# create a tempDF to store row combine two data frame
tempDF <-  rbind(pre_tweetsDF, cur_tweetsDF)
tempDF <- tempDF[, c("coder", "id", "long", "lat", "text")]
# Find the rows which have duplicates in a different group.
dupRows <- dupsBetweenGroups(tempDF, "coder")
# Print it alongside the data frame
tempDF <- cbind(tempDF, dup=dupRows)
# split two data frame, in ordering to find if there is duplicated in cur_tweetsDF
pre_tweetsDF <- subset(tempDF, coder=="A", select=-c(coder, dup))
cur_tweetsDF <- subset(tempDF, coder=="B", select = -coder)
# remove the row when dup== true, the row exists in pre_tweetsDF as well
cur_tweetsDF <- subset(cur_tweetsDF, dup=="FALSE")
cur_tweetsDF <- cur_tweetsDF[, c("id", "long", "lat", "text")]
#-----------step4: search the cur_tweetsDF geolocation inside the USA-----
cur_tweetsDF$long <- as.numeric(as.character(cur_tweetsDF$long))
cur_tweetsDF$lat <- as.numeric(as.character(cur_tweetsDF$lat))
cur_tweetsDF <- subset(cur_tweetsDF,cur_tweetsDF$long > (-180) & cur_tweetsDF$long < 0 & cur_tweetsDF$lat > 0 & cur_tweetsDF$lat < 180)
debug1 <- cur_tweetsDF
# cur_tweetsDF <- debug1
if(nrow(cur_tweetsDF) != 0){
# cur_tweetsDF$long <- as.character(cur_tweetsDF$long)
# cur_tweetsDF$lat <- as.character(cur_tweetsDF$lat)
geo1 <- getGeoLocation(cur_tweetsDF[1, "long"], cur_tweetsDF[1,"lat"])
if(nrow(cur_tweetsDF) > 1){
for (row in 2:nrow(cur_tweetsDF)) {
geon <- getGeoLocation(cur_tweetsDF[row, "long"], cur_tweetsDF[row, "lat"])
geo1 <- rbind(geo1, geon)
}
}
geo1 <- geo1[, c("state", "country")]
colnames(geo1)[1] <- "states"
cur_tweetsDF <- cur_tweetsDF[c("id","long", "lat", "text")]
cur_tweetsDF <-cbind(cur_tweetsDF,geo1)
}
cur_tweetsDF <- subset(cur_tweetsDF, cur_tweetsDF$country == "United States of America")
#-----------------------step5-1: add cur_tweetsDF to tweetsDF.csv files-------
temp1 <- cur_tweetsDF[, c("id", "long", "lat", "text")]
# combine two csv files
final1DF <- rbind(temp1, pre_tweetsDF)
# remove the data if they are all the same
dropX <-"X"
final1DF <-final1DF[ , !(names(final1DF) %in% dropX)]
write.csv(final1DF, file = "tweetsDF.csv")
#-----------------------step5-2: add cur_tweetsDF to updatedTweets.csv files-------
temp2 <- cur_tweetsDF[,c("id", "long", "lat", "states")]
pre_tweetsDF <- read.csv("updatedTweets.csv", header=T, sep=",")
dropX <-"X"
pre_tweetsDF <-pre_tweetsDF[ , !(names(pre_tweetsDF) %in% dropX)]
final2DF <- rbind(temp2, pre_tweetsDF)
write.csv(final2DF, file = "updatedTweets.csv")
